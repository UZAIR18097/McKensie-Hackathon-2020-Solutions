# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y8pj8wVJ94ycv5C6q__uiJrKr-R0ECYI
"""

#Importing the train data 
from google.colab import files
files.upload()

#Importing the test data
from google.colab import files
files.upload()

!pip install catboost

!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

"""Steps

1.   Problem Statement
2.   Importing Packages and Loading Data
1.   Explorartory Data Analysis
2.   Data Cleaning
1.   Applying PCA
2.   Modeling 
1.   Fine tuning parameters and deciding the final model
2.   Submit results

**Problem Statement**

1.   **To predict composite index for measurement of well being**
2.   **Optimal Energy Allocation to each galaxy and what rank is assigned to them**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pandas.plotting import scatter_matrix
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import IsolationForest
from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from pandas_profiling import ProfileReport
from sklearn.pipeline import FeatureUnion
from sklearn.preprocessing import PowerTransformer
from sklearn.decomposition import PCA
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from lightgbm import LGBMRegressor
from keras import models
from keras import layers
import tensorflow as tf
from keras import regularizers
# %matplotlib inline

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

df = pd.read_csv("train.csv")
print("The shape of data is {}".format(df.shape))

df.head()

df.info()

df.describe()

"""**DATA PREPROCESING STEPS**
1.  **All index values need to be btw 0 and 1 and all % need to be 0 and 100**

2.  **Following Columns which have negative values but cannot be**
*   Population,total(millions)
*   Population, ages 15–64 (millions)
*   Population, ages 65 and older (millions)
*   Population, under age 5 (millions)
*   Employment in agriculture (% of total employment)
*   Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)
*   Gross galactic product (GGP), total
*   Gross income per capita
*   Renewable energy consumption (% of total final energy consumption)
*   Natural resource depletion
*   Estimated gross galactic income per capita, female
*   Remittances, inflows (% of GGP)
*   Intergalactic inbound tourists (thousands)
*   Creature Immunodeficiency Disease prevalence, adult (% ages 15-49), total
3.   **Imputations of null values**
4.   **Outliers Removal**
5.   **Data Standardization**

*Exploratory Data Analysis*
"""

profile = ProfileReport(df, title ="Pandas Profiling Report",minimal=True)
profile.to_notebook_iframe()

profile.to_file("REPORT.html")

df.plot(kind = "scatter",x = "Estimated gross galactic income per capita, female", y ="y",title = "Scatter PLot of gross galactic income per capita",alpha =0.3)

df.plot(kind = "scatter",x = "Domestic credit provided by financial sector (% of GGP)", y ="y",title = "Scatter PLot of Domestic Credit Provided",alpha =0.3)

"""Some things to note: 

*   Some attributes have more than 80% of data missing
*   Some attributes have abnormal values such as negative values in population,income per capita,neg energy consumption etc which need to be taken care of.
*   Outliers which need to be treated

*   Feature Scaling is required since the different attributes have different scales
"""

#visualizing target variable
sns.set(style="whitegrid")
ax = sns.boxplot(x=df["y"])

plt.hist(x = df["y"])
plt.xlabel("Data")
plt.ylabel("Frequency")
plt.title("Histogram of Predictor")
plt.show()

"""Things to notice:

*   Most of the predictor's data is concentrated btw 0.02 and 0.2
*   The data is rightly skewed
"""

df.plot(kind = "scatter",x = "existence expectancy index", y ="y",title = "Scatter PLot of Existence Expectancy index and Predictor",alpha =0.3)

"""Some things to note:

*   Looks like y is capped from down side as there is a line y = 0.05.This may be becuause most values are btw this range.


*   A week positive co relation btw these two indicators
"""

#finding co relation with predictors
corr_mat = df.corr()
corr_mat["y"].sort_values(ascending = False)

"""Things to note:

*  Although Gender Inequality index has only 80% null values but it has   highest  negative corealtion.Certainly we cannot drop this feature. 
* The important features with respect to predictor are    

1.   Gender Inequality Index (GII)
2.   Intergalactic Development Index (IDI), Rank

1.   Intergalactic Development Index (IDI), male, Rank 
2.   Intergalactic Development Index (IDI), female, Rank 

1.   Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))
2.   Estimated gross galactic income per capita, female 

1.   Interstellar Data Net users, total (% of population)
2.   Intergalactic Development Index (IDI)

1.   Intergalactic Development Index (IDI), male
2.   Intergalactic Development Index (IDI), female 

1.   Education Index

Lets visualize only promising attributes now
"""

df = df.rename(columns={"Gender Inequality Index (GII)": "GII", "Intergalactic Development Index (IDI), Rank": "IDI","Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))": "Old","Estimated gross galactic income per capita, female": "Income/capita","Interstellar Data Net users, total (% of population)": "Data net",})
attributes = ["GII","IDI","Old","Income/capita","Data net"]
scatter_matrix(df[attributes],figsize=(12,8),diagonal="kde",alpha =0.8)
plt.show()

"""The most prominent relation can be seen btw Intergalactic Development Index (IDI), Rank and Gender Inequality Index(GII) which can be very helpful in prediction of required composite index

**Prepairing and cleaning data**
"""

df_labels = df.drop("y",axis =1)
df_labels.head()

df_labels.describe()

coloumn = df_labels["Gross income per capita"]
coloumn = coloumn.sort_values(ascending = False)
print(coloumn)
#df_labels["Gross income per capita"].value_counts(ascending = False)

df_predictor = df.loc[:,"y"]

df_predictor.shape

"""First remove abnormal values  like neg %"""

cols= ["Population, total (millions)","Population, ages 15–64 (millions)","Population, ages 65 and older (millions)","Population, under age 5 (millions)","Employment in agriculture (% of total employment)","Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)","Gross galactic product (GGP), total","Gross income per capita","Renewable energy consumption (% of total final energy consumption)","Natural resource depletion","Estimated gross galactic income per capita, female","Remittances, inflows (% of GGP)","Intergalactic inbound tourists (thousands)","Creature Immunodeficiency Disease prevalence, adult (% ages 15-49), total","Domestic credit provided by financial sector (% of GGP)"]
df[cols] = df[df[cols] > 0][cols]

coloumn = df["Gross income per capita"]
coloumn = coloumn.sort_values(ascending = False)
print(coloumn)
#just replaced neg % with nan

df.describe()

df_index_attrbiutes = df[["existence expectancy index","Income Index","Intergalactic Development Index (IDI)","Education Index","Intergalactic Development Index (IDI), female","Intergalactic Development Index (IDI), male","Gender Development Index (GDI)","Gender Inequality Index (GII)"]]

df_percent_attributes = df[["Population using at least basic drinking-water services (%)","Population using at least basic sanitation services (%)","Gross capital formation (% of GGP)","Population, urban (%)","Vulnerable employment (% of total employment)","Unemployment, total (% of labour force)","Employment in agriculture (% of total employment)","Labour force participation rate (% ages 15 and older)","Labour force participation rate (% ages 15 and older), female","Employment in services (% of total employment)","Labour force participation rate (% ages 15 and older), male","Employment to population ratio (% ages 15 and older)","Jungle area (% of total land area)","Share of employment in nonagriculture, female (% of total employment in nonagriculture)","Unemployment, youth (% ages 15–24)","Infants lacking immunization, red hot disease (% of one-galactic year-olds)","Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)","Outer Galaxies direct investment, net inflows (% of GGP)","Share of seats in senate (% held by female)","Renewable energy consumption (% of total final energy consumption)","Rural population with access to electricity (%)","Domestic credit provided by financial sector (% of GGP)","Population with at least some secondary education, female (% ages 25 and older)","Population with at least some secondary education, male (% ages 25 and older)","Gross fixed capital formation (% of GGP)","Remittances, inflows (% of GGP)","Population with at least some secondary education (% ages 25 and older)","Gross enrolment ratio, primary (% of primary under-age population)","Interstellar Data Net users, total (% of population)","Current health expenditure (% of GGP)","Creature Immunodeficiency Disease prevalence, adult (% ages 15-49), total","Private galaxy capital flows (% of GGP)"]]

#making pipeline for index attrbiutes
index_attributes_pipeline = Pipeline([
                                ('imputer', IterativeImputer( initial_strategy='median',random_state=0)),
                                ("scaler", MinMaxScaler(feature_range = (0,1))),
])
df_index_att_tr = index_attributes_pipeline.fit_transform(df_index_attrbiutes)

#making pipeline for % attributes
per_attributes_pipeline = Pipeline([
                                    ('imputer', IterativeImputer(initial_strategy='median',random_state=0)),
                                    ("scaler", MinMaxScaler(feature_range = (0,100))),                                    
])
df_percent_att_tr = per_attributes_pipeline.fit_transform(df_percent_attributes)

df_percent_att_tr

df_remaining = df.drop(["galaxy","galactic year","existence expectancy index","Income Index","Intergalactic Development Index (IDI)","Education Index","Intergalactic Development Index (IDI), female","Intergalactic Development Index (IDI), male","Gender Development Index (GDI)","Gender Inequality Index (GII)","Population using at least basic drinking-water services (%)","Population using at least basic sanitation services (%)","Gross capital formation (% of GGP)","Population, urban (%)","Vulnerable employment (% of total employment)","Unemployment, total (% of labour force)","Employment in agriculture (% of total employment)","Labour force participation rate (% ages 15 and older)","Labour force participation rate (% ages 15 and older), female","Employment in services (% of total employment)","Labour force participation rate (% ages 15 and older), male","Employment to population ratio (% ages 15 and older)","Jungle area (% of total land area)","Share of employment in nonagriculture, female (% of total employment in nonagriculture)","Unemployment, youth (% ages 15–24)","Infants lacking immunization, red hot disease (% of one-galactic year-olds)","Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)","Outer Galaxies direct investment, net inflows (% of GGP)","Share of seats in senate (% held by female)","Renewable energy consumption (% of total final energy consumption)","Rural population with access to electricity (%)","Domestic credit provided by financial sector (% of GGP)","Population with at least some secondary education, female (% ages 25 and older)","Population with at least some secondary education, male (% ages 25 and older)","Gross fixed capital formation (% of GGP)","Remittances, inflows (% of GGP)","Population with at least some secondary education (% ages 25 and older)","Gross enrolment ratio, primary (% of primary under-age population)","Interstellar Data Net users, total (% of population)","Current health expenditure (% of GGP)","Creature Immunodeficiency Disease prevalence, adult (% ages 15-49), total","Private galaxy capital flows (% of GGP)","y"],axis =1)

df_remaining.shape

df_remaining.describe()

#making pipeline for remaining attrbiutes
remaining_col = Pipeline([
                                ('imputer', IterativeImputer( initial_strategy='median',random_state=0)),
])
df_remaining_att_tr = remaining_col.fit_transform(df_remaining)

df_remaining_att_tr

#A simple custom transformer for selecting attributes
from sklearn.base import BaseEstimator, TransformerMixin
class DataFrameSelector(BaseEstimator, TransformerMixin):
  def __init__(self, attribute_names):
    self.attribute_names = attribute_names
  def fit(self, X, y=None):
    return self
  def transform(self, X):
    return X[self.attribute_names].values

"""Joining the three pipelines"""

from sklearn.pipeline import FeatureUnion
#three arrays of data
df_data_cols= list(df_index_attrbiutes)
df_data_per = list(df_percent_attributes)
df_data_rem = list(df_remaining)
#three pipelines
index_pipeline =  Pipeline([
                                ('selector', DataFrameSelector(df_data_cols)), 
                                ('imputer', IterativeImputer( initial_strategy='median',random_state=0)),
                                ("scaler", MinMaxScaler(feature_range = (0,1))),
])

percent_pipeline = Pipeline([        
                                    ('selector', DataFrameSelector(df_data_per)),
                                    ('imputer', IterativeImputer(initial_strategy='median',random_state=0)),
                                    ("scaler", MinMaxScaler(feature_range = (0,100))),                                    
])

remaining_pipeline =  Pipeline([ 
                                ('selector', DataFrameSelector(df_data_rem)),
                                ('imputer', IterativeImputer( initial_strategy='median',random_state=0)),
])
#full pipeline
full_pipeline = FeatureUnion(transformer_list=[
 ("index_pipeline", index_pipeline),
 ("percent_pipeline", percent_pipeline),
 ("remaining_pipeline",remaining_pipeline)
 ])
data_prepared = full_pipeline.fit_transform(df_labels) #passing the whole data of 77 features and 3865 samples.Just dropped the first two columns and last predictor
data_prepared

df_labels = df_labels.drop(["galaxy","galactic year"],axis =1)

data_prepared_df = pd.DataFrame(data=data_prepared,columns=df_labels.columns)

data_prepared_df.head()

data_prepared.shape

df_imp.skew()

"""Since some columns still have high skewness I will Power transform them using Sklearn"""

#plot histogram
ax = sns.distplot(df_imp['Vulnerable employment (% of total employment)'])
ax.set(xlabel ="Vulnerable employment(% of total employment)",ylabel = "Counts" )
plt.show()
#df_imp[]= np.log(df_imp["Vulnerable employment (% of total employment)"])
#rint(crim)

data = df_imp
from sklearn.preprocessing import PowerTransformer
#instatiate 
pt = PowerTransformer(method='yeo-johnson') 

#Fit the data to the powertransformer
skl_yeojohnson = pt.fit(data)

#Lets get the Lambdas that were found
print (skl_yeojohnson.lambdas_)

calc_lambdas = skl_yeojohnson.lambdas_

#Transform the data 
skl_yeojohnson = pt.transform(data)

#Pass the transformed data into a new dataframe 
df_xt = pd.DataFrame(data=skl_yeojohnson, columns=data.columns)

df_xt.head()

ax = sns.distplot(df_xt['Vulnerable employment (% of total employment)'])
ax.set(xlabel ="Vulnerable employment(% of total employment)",ylabel = "Counts" )
plt.show()

df_xt.skew()

"""Considerable improvement in skewness.Finally make the pipe line."""

scaler = StandardScaler()
power = PowerTransformer(method='yeo-johnson')
transformer_pipeline = Pipeline(steps=[('s', scaler), ('p', power)])
transformed_data = transformer_pipeline.fit_transform(df_labels)
transformed_df = pd.DataFrame(data = transformed_data,columns=df_imp.columns)

transformed_df.skew()

"""Using Standard Scaler with Power transformer gave better results.Lastly the objective is to run the all the steps in big data pipeline and runnin all the steps"""

df = pd.read_csv("train.csv")

df_labels = df.drop("y",axis =1)

df_labels = df_labels.drop(["galaxy","galactic year"],axis =1)

seed_value = 0
import random
random.seed(seed_value)

from sklearn.pipeline import FeatureUnion
#three arrays of data
df_data_cols= list(df_index_attrbiutes)
df_data_per = list(df_percent_attributes)
df_data_rem = list(df_remaining)
#three pipelines
index_pipeline =  Pipeline([
                                ('selector', DataFrameSelector(df_data_cols)), 
                                ('imputer', IterativeImputer( initial_strategy='median',random_state=0)),
                                ("scaler", MinMaxScaler(feature_range = (0,1))),
])

percent_pipeline = Pipeline([        
                                    ('selector', DataFrameSelector(df_data_per)),
                                    ('imputer', IterativeImputer(initial_strategy='median',random_state=0)),
                                    ("scaler", MinMaxScaler(feature_range = (0,100))),                                    
])

remaining_pipeline =  Pipeline([ 
                                ('selector', DataFrameSelector(df_data_rem)),
                                ('imputer', IterativeImputer( initial_strategy='median',random_state=0)),
])
#full pipeline
full_pipeline = FeatureUnion(transformer_list=[
 ("index_pipeline", index_pipeline),
 ("percent_pipeline", percent_pipeline),
 ("remaining_pipeline",remaining_pipeline)
 ])
partial_data_prepared = full_pipeline.fit_transform(df_labels) #passing the whole data of 77 features and 3865 samples.Just dropped the first two columns and last predictor
partial_data_prepared

partial_data_prepared_df = pd.DataFrame(data = partial_data_prepared,columns = df_labels.columns)

tranformer_pipeline = Pipeline([
                                ('StandardScaler',  StandardScaler()), 
                                ('Powertransformer', PowerTransformer(method='yeo-johnson')), 
                                ])
Full_data_prepared = tranformer_pipeline.fit_transform(partial_data_prepared_df)

Full_data_prepared_df = pd.DataFrame(data = Full_data_prepared, columns = df_labels.columns)

Full_data_prepared_df.describe()

Full_data_prepared_df.shape

Full_data_prepared_df.head()

#joining predictor with preprocessed data
Full_data_prepared_df["y"] = df_predictor.values

Full_data_prepared_df.head()

Full_data_prepared_df.shape

"""Before data Preprocess"""

df.plot(kind = "scatter",x = "Estimated gross galactic income per capita, female", y ="Education Index",title = "Scatter PLot of gross galactic income per capita",alpha =0.3)

df.plot(kind = "scatter",x = "Estimated gross galactic income per capita, female", y ="y",title = "Scatter PLot of gross galactic income per capita",alpha =0.3)

"""After Data Preprocess"""

Full_data_prepared_df.plot(kind = "scatter",x = "Estimated gross galactic income per capita, female", y ="Education Index",title = "Scatter PLot of gross galactic income per capita",alpha =0.3)

Full_data_prepared_df.plot(kind = "scatter",x = "Estimated gross galactic income per capita, female", y ="y",title = "Scatter PLot of gross galactic income per capita",alpha =0.3)

Full_data_prepared_df.skew()

profile = ProfileReport(df_final_full, title ="After Data Preprocess",minimal=True)
profile.to_notebook_iframe()

profile.to_file("Preprocess report.html")

#saving the updated frame
from google.colab import files
Full_data_prepared_df.to_csv('Preprocessed_train.csv')
files.download('Preprocessed_train.csv')

"""Preprocessing the test data"""

test_data = pd.read_csv("test.csv")

test_data = test_data.drop(["galactic year","galaxy"],axis =1)

test_data.shape

#removing neg values from the
cols= ["Population, total (millions)","Population, ages 15–64 (millions)","Population, ages 65 and older (millions)","Population, under age 5 (millions)","Employment in agriculture (% of total employment)","Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)","Gross galactic product (GGP), total","Gross income per capita","Renewable energy consumption (% of total final energy consumption)","Natural resource depletion","Estimated gross galactic income per capita, female","Remittances, inflows (% of GGP)","Intergalactic inbound tourists (thousands)","Creature Immunodeficiency Disease prevalence, adult (% ages 15-49), total","Domestic credit provided by financial sector (% of GGP)"]
test_data[cols] = test_data[test_data[cols] > 0][cols]

test_data_transform = full_pipeline.transform(test_data)
test_data_transform_df = pd.DataFrame(data = test_data_transform, columns = test_data.columns) 
full_data_transform = tranformer_pipeline.transform(test_data_transform_df)
full_data_test_df = pd.DataFrame(data = full_data_transform, columns = test_data.columns)

full_data_test_df.describe()

#saving the updated frame
from google.colab import files
full_data_test_df.to_csv('Preprocessed_test.csv')
files.download('Preprocessed_test.csv')

"""Applying PCA and Visualizing results"""

df = pd.read_csv("Preprocessed_train .csv")
df_labels = df.drop(["Unnamed: 0","y"],axis=1)
df_predictor = df.iloc[:,-1]
print("The shape of label data is {}".format(df_labels.shape))
print("The shape of predictor is {}".format(df_predictor.shape))

df_labels.head()

pca = PCA(n_components=2)
pca.fit(df_labels)

# transform data onto the first two principal components
X_pca = pca.transform(df_labels)
print("Original shape:{}".format(df_labels.shape))
print("Original shape:{}".format(X_pca.shape))

ratio = pca.explained_variance_ratio_
print("Explained Variation per principal component: {}".format(ratio))

PCA_df = pd.DataFrame(data=X_pca,columns=['Principal Component - 1','Principal Component - 2'])
PCA_df.head()

#plotting the components
fig = plt.figure(figsize = (8,8))
plt.scatter(PCA_df["Principal Component - 1"],PCA_df["Principal Component - 2"])
plt.xlabel('Principal Component - 1',fontsize=20)
plt.ylabel('Principal Component - 2',fontsize=20)
plt.title("Principal Component Analysis of data",fontsize=20)
plt.plot()

#visualizing on all data for Explained variance
pca = PCA()
pca.fit(df_labels)
var = pca.explained_variance_ratio_
var1=np.cumsum(np.round(var, decimals=4)*100)
print(var1)
plt.ylabel("Propotion of Varaince Explained")
plt.xlabel("Number of Principal Component")
plt.title("Scree plot")
plt.plot(var,"ro")

var1=np.cumsum(np.round(var, decimals=4)*100)
plt.ylabel("CumulativePropotion of Varaince Explained")
plt.xlabel("Number of Principal Components")
plt.title("Cumulative Scree plot")
plt.plot(var1,"b")

"""95 % of cumulative propotion Variance can be preserved in only 36 principal components,reducing data dimensionality to half

**Modeling**

STRATEGY

1.   Split data randomly on train (60%),validate (20%) and test (20%) 
2.   Tree Based Models with and without Data Preprocessing
1.   Neural Networks
"""

#with preprocessing
df_preprocessed = pd.read_csv("Preprocessed_train .csv")
X_pp = df_preprocessed.drop(["Unnamed: 0","y"],axis=1)
y = df_preprocessed.iloc[:,-1]
print(X_pp.shape)
x_pp_train,x_pp_test,y_pp_train,y_pp_test = train_test_split(X_pp,y,test_size = 0.2,random_state = 0)
print(x_pp_train.shape)
print(x_pp_test.shape)

#without preprocessing
df = pd.read_csv("train.csv")
X = df.drop(["galactic year","galaxy","y"],axis=1) 
print(X.shape)
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)

#writing function for evaluation of scores
def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standard deviation:", scores.std())

#applying Random Forest Regressor without Preprocessed
pipe = make_pipeline(IterativeImputer(),RandomForestRegressor(n_estimators=100,random_state =0))
pipe.fit(x_train,y_train)  
print("Train score: {:.2f}".format(pipe.score(x_train, y_train))) 
print("Test score: {:.2f}".format(pipe.score(x_test, y_test)))

scores = cross_val_score(pipe,x_train,y_train,scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-scores)
display_scores(rmse_scores)

"""Applying Random Forest Regressor with cross validation= 5 on preprocessed data"""

forest = RandomForestRegressor(n_estimators=100,random_state =0)
scores = cross_val_score(forest, x_pp_train, y_pp_train,scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-scores)
display_scores(rmse_scores)

forest.fit(x_pp_train,y_pp_train)
print("Accuracy on training set: {:.3f}".format(forest.score(x_pp_train, y_pp_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(x_pp_test, y_pp_test)))

"""Shows that model is severly overfitting(High Variance) but preprocessed data is slightly better and can give better results"""

#Seeing the Feature Importance by Mean Decrease in Impurity (MDI)
features = x_pp_train.columns
importances = forest.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize=(20,100))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""Applying Gradient Boosting Regressor"""

gbr = GradientBoostingRegressor(random_state=0)
scores = cross_val_score(gbr, x_pp_train, y_pp_train,scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-scores)
display_scores(rmse_scores)

gbr.fit(x_pp_train,y_pp_train)
print("Accuracy on training set: {:.3f}".format(gbr.score(x_pp_train, y_pp_train)))
print("Accuracy on test set: {:.3f}".format(gbr.score(x_pp_test, y_pp_test)))

#Seeing the Feature Importance by Mean Decrease in Impurity (MDI)
features = x_pp_train.columns
importances = gbr.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize=(20,100))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""Applying Light GBM"""

lgbm = LGBMRegressor(random_state=0)
scores = cross_val_score(lgbm, x_pp_train, y_pp_train,scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-scores)
display_scores(rmse_scores)

#tuning
from sklearn.model_selection import GridSearchCV
param_grid = {'n_estimators': [100, 500, 1000,],
              'max_depth': [-2,-1,5,7],
              'num_leaves' : [10,100,500,],
              'learning_rate' : [0.001,0.01,0.1],
}
gridsearch = GridSearchCV(LGBMRegressor(),param_grid,cv =5,scoring = "neg_root_mean_squared_error",n_jobs =-1)
gridsearch.fit(x_pp_train,y_pp_train)
best_params = gridsearch.best_params_
model = gridsearch.best_estimator_
score = gridsearch.best_score_
rmse_scores = (-score)
#for item in grid.grid_scores_:
    #print ("\t%s %s %s" % ('\tGRIDSCORES\t',  "R" , item))
#print ('%s\tHP\t%s\t%f' % ("R" , str(best_params) ,abs(score)))
print(best_params)
print(rmse_scores)
print(model)

from sklearn.metrics import mean_squared_error


lgbm = LGBMRegressor(random_state=0,learning_rate= 0.1, max_depth= 4, n_estimators = 100, num_leaves= 30,min_data_in_leaf=10,max_bin = 100,lambda_l1 = 0.001,lambda_l2 = 0.001,feature_fraction = 0.8,bagging_fraction=0.6)
lgbm.fit(x_pp_train,y_pp_train)
print("Accuracy on training set: {:.3f}".format(lgbm.score(x_pp_train, y_pp_train)))
print("Accuracy on test set: {:.3f}".format(lgbm.score(x_pp_test, y_pp_test)))
y_pred = lgbm.predict(x_pp_test)
y_pred_train = lgbm.predict(x_pp_train)
print("RMSE on train :{:.3f}".format(mean_squared_error(y_pp_train, y_pred_train, squared=False)))
print("RMSE on test :{:.3f}".format(mean_squared_error(y_pp_test, y_pred, squared=False)))

#Seeing the Feature Importance by Mean Decrease in Impurity (MDI)
features = x_pp_train.columns
importances = lgbm.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize=(20,100))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""Making a model from top 10 attributes from LGBM"""

X = X_pp[["existence expectancy at birth","Education Index","Gross galactic product (GGP), total","Outer Galaxies direct investment, net inflows (% of GGP)","existence expectancy index","Mean years of education, male (galactic years)","Income Index","Interstellar Data Net users, total (% of population)","Interstellar phone subscriptions (per 100 people)","Adjusted net savings "]]
#random split
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)
lgbm = LGBMRegressor(random_state=0,learning_rate= 0.1, max_depth= 5, n_estimators = 100, num_leaves= 100)
lgbm.fit(x_train,y_train)
print("R2 on training set: {:.3f}".format(lgbm.score(x_train, y_train)))
print("R2 on test set: {:.3f}".format(lgbm.score(x_test, y_test)))
y_pred = lgbm.predict(x_test)
y_pred_train = lgbm.predict(x_train)
print("RMSE on train :{:.3f}".format(mean_squared_error(y_train, y_pred_train, squared=False)))
print("RMSE on test :{:.3f}".format(mean_squared_error(y_test, y_pred, squared=False)))

#predicting on test data
test_data = pd.read_csv("Preprocessed_test.csv")
test_data = test_data.drop("Unnamed: 0",axis =1)
#predicting model on test data
prediction = lgbm.predict(test_data)
prediction_df = pd.DataFrame(data = prediction)
#saving predictions to csv file
prediction_df.to_csv("lightgbm.csv")

"""Interesting that Feature importance of LGBM is completely different from Gradient Boosting and Random Forest

Applying Extra Trees Regressor

Now applying Xtreme Gradient Boosting Machine
"""

from xgboost import XGBRegressor
xgbr = XGBRegressor(random_state = 0)
scores = cross_val_score(xgbr, x_pp_train, y_pp_train,scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-scores)
display_scores(rmse_scores)

"""Applying Catboost Regressor"""

from catboost import CatBoostRegressor
cbr = CatBoostRegressor(random_state = 0)
scores = cross_val_score(cbr, x_pp_train, y_pp_train,scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-scores)
display_scores(rmse_scores)

"""CAT boost Regressor and LGBM regressor had best performance

**Applying Neural Network**
"""

#with preprocessing
df_preprocessed = pd.read_csv("Preprocessed_train .csv")
X = df_preprocessed.drop(["Unnamed: 0","y"],axis=1)
y = df_preprocessed.iloc[:,-1]
print("Shape of whole data is {}".format(X.shape))
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)
print("Shape of training data is {}".format(X_train.shape))
print("Shape of testing data is {}".format(X_test.shape))

#validating my approach
x_val = X_train[:600]
partial_x_train = X_train[600:]
y_val = y_train[:600]
partial_y_train = y_train[600:]

import tensorflow as tf
#defining model structure
def build_model():
  model = models.Sequential()
  model.add(layers.Dense(32,activation = "relu",input_shape = (X_train.shape[1],)))
  model.add(layers.Dense(32,activation = "relu"))
  model.add(layers.Dense(32,activation = "relu"))
  model.add(layers.Dense(1))
  model.compile(optimizer = "rmsprop",loss = "mse",metrics=[tf.keras.metrics.RootMeanSquaredError()])
  return model

#fitting model
model = build_model()
num_epochs = 200
history = model.fit(partial_x_train, 
                    partial_y_train,
                    validation_data=(x_val, y_val),
                    epochs=num_epochs, 
                    batch_size=10, 
                    verbose=1)

model.summary()

history.history.keys()

plt.plot(history.history['loss'] ,'bo')
plt.plot(history.history['val_loss'],'b',)
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['training loss', 'validation loss'], loc='upper right')
plt.show()

"""This shows overfitting right from the start"""

plt.plot(history.history['root_mean_squared_error'],"bo")
plt.plot(history.history['val_root_mean_squared_error'],"b")
plt.title('Training and validation accuracy')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Training_rmse', 'Validation_rmse'], loc='best')
plt.show()

"""Applying Standard Scaler to make convergence faster"""

sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)
X_train.head()

print(X_train.shape)
print(X_test.shape)

#validating my approach
x_val = X_train[:600]
partial_x_train = X_train[600:]
y_val = y_train[:600]
partial_y_train = y_train[600:]

import tensorflow as tf
#defining model structure
def build_model():
  model = models.Sequential()
  model.add(layers.Dense(32,activation = "relu",input_shape = (X_train.shape[1],)))
  model.add(layers.Dense(32,activation = "relu"))
  model.add(layers.Dense(32,activation = "relu"))
  model.add(layers.Dense(1))
  model.compile(optimizer = "rmsprop",loss = "mse",metrics=[tf.keras.metrics.RootMeanSquaredError()])
  return model

#fitting model
model = build_model()
num_epochs = 200
history = model.fit(partial_x_train, 
                    partial_y_train,
                    validation_data=(x_val, y_val),
                    epochs=num_epochs, 
                    batch_size=10, 
                    verbose=1)

plt.plot(history.history['loss'] ,'bo')
plt.plot(history.history['val_loss'],'b',)
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['training loss', 'validation loss'], loc='upper right')
plt.show()

plt.plot(history.history['root_mean_squared_error'],"bo")
plt.plot(history.history['val_root_mean_squared_error'],"b")
plt.title('Training and validation accuracy')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Training_rmse', 'Validation_rmse'], loc='best')
plt.show()

"""Adding L2 weight regularization to the model"""

import tensorflow as tf
from keras import regularizers
#defining model structure
def build_model():
  model = models.Sequential()
  model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = "relu",input_shape = (X_train.shape[1],)))
  model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(1))
  model.compile(optimizer = "rmsprop",loss = "mse",metrics=[tf.keras.metrics.RootMeanSquaredError()])
  return model

#fitting model
model = build_model()
num_epochs = 200
history = model.fit(partial_x_train, 
                    partial_y_train,
                    validation_data=(x_val, y_val),
                    epochs=num_epochs, 
                    batch_size=10, 
                    verbose=1)

plt.plot(history.history['loss'] ,'bo')
plt.plot(history.history['val_loss'],'b',)
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['training loss', 'validation loss'], loc='upper right')
plt.show()

plt.plot(history.history['root_mean_squared_error'],"bo")
plt.plot(history.history['val_root_mean_squared_error'],"b")
plt.title('Training and validation accuracy')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Training_rmse', 'Validation_rmse'], loc='best')
plt.show()

"""The problem is solved but we can add more complexity in network"""

#with preprocessing and standardizing
df_preprocessed = pd.read_csv("Preprocessed_train .csv")
X = df_preprocessed.drop(["Unnamed: 0","y"],axis=1)
y = df_preprocessed.iloc[:,-1]
print("Shape of whole data is {}".format(X.shape))
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)
print("Shape of training data is {}".format(X_train.shape))
print("Shape of testing data is {}".format(X_test.shape))

sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)
X_train.head()

#defining model structure
def build_model():
  model = models.Sequential()
  model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = "relu",input_shape = (X_train.shape[1],)))
  model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(1))
  model.compile(optimizer = "rmsprop",loss = "mse",metrics=[tf.keras.metrics.RootMeanSquaredError()])
  return model

#fitting model
model = build_model()
num_epochs = 200
history = model.fit(partial_x_train, 
                    partial_y_train,
                    validation_data=(x_val, y_val),
                    epochs=num_epochs, 
                    batch_size=10, 
                    verbose=1)

plt.plot(history.history['loss'] ,'bo')
plt.plot(history.history['val_loss'],'b',)
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['training loss', 'validation loss'], loc='upper right')
plt.show()

plt.plot(history.history['root_mean_squared_error'],"bo")
plt.plot(history.history['val_root_mean_squared_error'],"b")
plt.title('Training and validation accuracy')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Training_rmse', 'Validation_rmse'], loc='best')
plt.show()

model.save("Hackathon_Neural_Net.h5")

model = tf.keras.models.load_model('Hackathon_Neural_Net.h5')

#predicting on remaining test data
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
y_hat = model.predict(X_test)
Rmse_test_data = mean_squared_error(y_test,y_hat,squared =False)
R2_score = r2_score(y_test,y_hat)
print("RMSE on Test data is {}".format(Rmse_test_data))
print("R2 on Test data is {}".format(R2_score))
#plotting
plt.scatter(y_test,y_hat)
plt.show()

"""Getting around 0.032 is the lowest NN can get without over fitting but we can train for even more layers and bigger to see where we can go

**Trying same model with different scaler**
"""

#with preprocessing and standardizing
df_preprocessed = pd.read_csv("Preprocessed_train .csv")
X = df_preprocessed.drop(["Unnamed: 0","y"],axis=1)
y = df_preprocessed.iloc[:,-1]
print("Shape of whole data is {}".format(X.shape))
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)
print("Shape of training data is {}".format(X_train.shape))
print("Shape of testing data is {}".format(X_test.shape))

sc_X = MinMaxScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)
X_train.head()

#validating my approach
x_val = X_train[:600]
partial_x_train = X_train[600:]
y_val = y_train[:600]
partial_y_train = y_train[600:]

#defining model structure
def build_model():
  model = models.Sequential()
  model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = "relu",input_shape = (X_train.shape[1],)))
  model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(1))
  model.compile(optimizer = "rmsprop",loss = "mse",metrics=[tf.keras.metrics.RootMeanSquaredError()])
  return model

#fitting model
model = build_model()
num_epochs = 200
history = model.fit(partial_x_train, 
                    partial_y_train,
                    validation_data=(x_val, y_val),
                    epochs=num_epochs, 
                    batch_size=10, 
                    verbose=1)

plt.plot(history.history['loss'] ,'bo')
plt.plot(history.history['val_loss'],'b',)
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['training loss', 'validation loss'], loc='upper right')
plt.show()

plt.plot(history.history['root_mean_squared_error'],"bo")
plt.plot(history.history['val_root_mean_squared_error'],"b")
plt.title('Training and validation Rmse')
plt.ylabel('RMSE')
plt.xlabel('Epochs')
plt.legend(['Training_rmse', 'Validation_rmse'], loc='best')
plt.show()

"""Results by standard scaler are better
Lets try reducing complexity and reduce features by PCA and apply on NN
"""

#with preprocessing and standardizing
df_preprocessed = pd.read_csv("Preprocessed_train .csv")
X = df_preprocessed.drop(["Unnamed: 0","y"],axis=1)
y = df_preprocessed.iloc[:,-1] 

sc_X = StandardScaler()
X_scaled = sc_X.fit_transform(X)

pca = PCA(0.95)
pca.fit(X_scaled)
X_pca = pca.transform(X_scaled)
X = pd.DataFrame(data = X_pca)
X.head()

#seperating into train and test
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)
print("Shape of training data is {}".format(X_train.shape))
print("Shape of testing data is {}".format(X_test.shape))

#validating my approach
x_val = X_train[:600]
partial_x_train = X_train[600:]
y_val = y_train[:600]
partial_y_train = y_train[600:]

#defining model structure
def build_model():
  model = models.Sequential()
  model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu",input_shape = (X_train.shape[1],)))
  model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  #model.add(layers.Dense(16,kernel_regularizer=regularizers.l2(0.001),activation = "relu"))
  model.add(layers.Dense(1))
  model.compile(optimizer = "rmsprop",loss = "mse",metrics=[tf.keras.metrics.RootMeanSquaredError()])
  return model

#fitting model
model = build_model()
num_epochs = 2000
history = model.fit(partial_x_train, 
                    partial_y_train,
                    validation_data=(x_val, y_val),
                    epochs=num_epochs, 
                    batch_size=10, 
                    verbose=1)

model.save("Hackathon_PCA_Neurals.h5")

plt.plot(history.history['loss'] ,'bo')
plt.plot(history.history['val_loss'],'b',)
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['training loss', 'validation loss'], loc='upper right')
plt.show()

plt.plot(history.history['root_mean_squared_error'],"bo")
plt.plot(history.history['val_root_mean_squared_error'],"b")
plt.title('Training and validation Rmse')
plt.ylabel('RMSE')
plt.xlabel('Epochs')
plt.legend(['Training_rmse', 'Validation_rmse'], loc='best')
plt.show()

#predicting on remaining test data
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
y_hat = model.predict(X_test)
Rmse_test_data = mean_squared_error(y_test,y_hat,squared =False)
R2_score = r2_score(y_test,y_hat)
print("RMSE on Test data is {}".format(Rmse_test_data))
print("R2 on Test data is {}".format(R2_score))
#plotting
plt.scatter(y_test,y_hat)
plt.show()

"""Saving first Predictions on Test Data provided by Hackathon"""

#predicting on test data
test_data = pd.read_csv("Preprocessed_test.csv")
test_data = test_data.drop("Unnamed: 0",axis =1)
test_scaled = sc_X.transform(test_data)
test_pca = pca.transform(test_scaled)
test_pca.shape

test_pca_df = pd.DataFrame(data = test_pca)
test_pca_df.head()

#predicting model on test data
prediction = model.predict(test_pca_df)
prediction_df = pd.DataFrame(data = prediction)
#saving predictions to csv file
prediction_df.to_csv("Neural Network Prediction.csv")